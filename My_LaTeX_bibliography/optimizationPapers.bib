##### NUMERICAL OPTIMIZATION BOOKS AND CLASSIC RESULTS

@book{Nocedal2012,
author = {Nocedal, Jorge and Wright, Stephen J.},
booktitle = {Advances in Industrial Control},
doi = {10.1007/978-1-4471-2224-1_2},
edition = {Second},
isbn = {9780387303031},
issn = {21931577},
number = {9781447122234},
publisher = {Springer},
title = {{Numerical optimization}},
year = {2012}
}

@article{Armijo1966,
author = {Armijo, Larry},
doi = {10.2140/pjm.1966.16.1},
issn = {0030-8730},
journal = {Pacific Journal of Mathematics},
month = {jan},
number = {1},
pages = {1--3},
title = {{Minimization of functions having Lipschitz continuous first partial derivatives}},
volume = {16},
year = {1966}
}

@article{Wolfe1969,
author = {Wolfe, Philip},
doi = {10.1137/1011036},
issn = {0036-1445},
journal = {SIAM Review},
month = {apr},
number = {2},
pages = {226--235},
title = {{Convergence Conditions for Ascent Methods}},
volume = {11},
year = {1969}
}


@article{Wolfe1971,
author = {Wolfe, Philip},
doi = {10.1137/1013035},
issn = {0036-1445},
journal = {SIAM Review},
month = {apr},
number = {2},
pages = {185--188},
title = {{Convergence Conditions for Ascent Methods. II: Some Corrections}},
volume = {13},
year = {1971}
}


##### MULTI-START

@Inbook{
Marti2003,
author={Mart{\'i}, Rafael},
editor={Glover, Fred and Kochenberger, Gary A.},
title={Multi-Start Methods},
bookTitle={Handbook of Metaheuristics},
year={2003},
publisher={Springer US},
address={Boston, MA},
pages={355--368},
abstract={Heuristic search procedures that aspire to find global optimal solutions to hard combinatorial optimization problems usually require some type of diversification to overcome local optimality. One way to achieve diversification is to re-start the procedure from a new solution once a region has been explored. In this chapter we describe the best known multi-start methods for solving optimization problems. We propose classifying these methods in terms of their use of randomization, memory and degree of rebuild. We also present a computational comparison of these methods on solving the linear ordering problem in terms of solution quality and diversification power.},
isbn={978-0-306-48056-0},
doi={10.1007/0-306-48056-5\_12},
}
% url={https://doi.org/10.1007/0-306-48056-5\_12}

@article{Schoen1991,
abstract = {In this paper stochastic algorithms for global optimization are reviewed. After a brief introduction on random-search techniques, a more detailed analysis is carried out on the application of simulated annealing to continuous global optimization. The aim of such an analysis is mainly that of presenting recent papers on the subject, which have received only scarce attention in the most recent published surveys. Finally a very brief presentation of clustering techniques is given. {\textcopyright} 1991 Kluwer Academic Publishers.},
author = {Schoen, Fabio},
doi = {10.1007/BF00119932},
file = {:Volumes/Mac_Fra_Maxtor/PhD TO/PhD doc/Mendeley/PDFforMendeley/schoen1991.pdf:pdf},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Stochastic algorithms,clustering,simulated annealing},
number = {3},
pages = {207--228},
title = {{Stochastic techniques for global optimization: A survey of recent advances}},
volume = {1},
year = {1991}
}

@techreport{Hu94randomrestarts,
    author = {X. Hu and M. C. Spruill and R. Shonkwiler and R. Shonkwiler},
    title = {Random Restarts in Global Optimization},
    institution = {Georgia Institute of Technology},
    address = {Atlanta},
    year = {1994}
}


@article{MIGDALAS2003375,
title = {Nonlinear optimization and parallel computing},
journal = {Parallel Computing},
volume = {29},
number = {4},
pages = {375-391},
year = {2003},
note = {Parallel computing in numerical optimization},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(03)00013-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167819103000139},
author = {A. Migdalas and G. Toraldo and V. Kumar},
keywords = {Parallel computing, Quadratic programming, Interior point methods, Global optimization, Heuristics},
abstract = {The new computational technologies are having a very strong influence on numerical optimization, in several different ways. Many researchers have been stimulated by the need to either conform the existing numerical techniques to the new parallel architectures or to devise completely new parallel solution approaches. A mini-symposium on Parallel Computing in Nonlinear Optimization was held in Naples, Italy, September 2001, during the International Conference ParCo2001, in order to bring together researchers active in this field and to discuss and share their findings. Some of the papers presented during the mini-symposium, as well as additional contributions from other researchers are collected in this special issue. Clearly, two different trends, well representative for most of the current research activities, can be identified. Firstly, there is an attempt to encapsulate parallel linear algebra software and algorithms into optimization codes, particularly codes implementing interior point strategies for which the linear algebra issues are very critical, and secondly, there is an effort to devise new parallel solution strategies in global optimization, either for specific or general purpose problems, motivated by the large size and the combinatorial nature of them. In the present paper we review the literature on these trends and classify the contributed papers within this framework.}
}

@article{Dixon1993,
abstract = {In this paper, we report results of implementations of algorithms designed (i) to solve the global optimization problem (GOP) and (ii) to run on a parallel network of transputers. There have always been two alternative approaches to the solution of the GOP, probabilistic and deterministic. Interval methods can be implemented on our network of transputers using Concurrent ADA, and a secondary objective of the tests reported was to investigate the relative computer times required by parallel interval algorithms compared to probabilistic methods. {\textcopyright} 1993 Plenum Publishing Corporation.},
author = {Dixon, L. C.W. and Jha, M.},
doi = {10.1007/BF00940587},
file = {:Volumes/Mac_Fra_Maxtor/PhD TO/PhD doc/Mendeley/PDFforMendeley/dixon1993.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Global optimization,interval methods,parallel computing,transputer performance},
number = {2},
pages = {385--395},
title = {{Parallel algorithms for global optimization}},
volume = {79},
year = {1993}
}



@article{Mathesen2021,
abstract = {A common approach to global optimization is to combine local optimization methods with random restarts. Restarts have been used as a performance boosting approach. They can be a means to avoid “slow progress” by exploiting a potentially good solution, and restarts can enable the potential discovery of multiple local solutions, thus improving the overall quality of the returned solution. A multi-start method is a way to integrate local and global approaches; where the global search itself can be used to restart a local search. Bayesian optimization methods aim to find global optima of functions that can only be point-wise evaluated by means of a possibly expensive oracle. We propose the stochastic optimization with adaptive restart (SOAR) framework, that uses the predictive capability of Gaussian process models as a means to adaptively restart local search and intelligently select restart locations with current information. This approach attempts to balance exploitation with exploration of the solution space. We study the asymptotic convergence of SOAR to a global optimum, and empirically evaluate SOAR performance through a specific implementation that uses the Trust Region method as the local search component. Numerical experiments show that the proposed algorithm outperforms existing methodologies over a suite of test problems of varying problem dimension with a finite budget of function evaluations.},
author = {Mathesen, Logan and Pedrielli, Giulia and Ng, Szu Hui and Zabinsky, Zelda B.},
doi = {10.1007/s10898-020-00937-5},
file = {:Volumes/Mac_Fra_Maxtor/PhD TO/PhD doc/Mendeley/PDFforMendeley/Mathesen2021_Article_StochasticOptimizationWithAdap.pdf:pdf},
issn = {15732916},
journal = {Journal of Global Optimization},
keywords = {Black-box optimization,Local restart,Stochastic search,Surrogate modeling},
number = {1},
pages = {87--110},
publisher = {Springer US},
title = {{Stochastic optimization with adaptive restart: a framework for integrated local and global learning}},
url = {https://doi.org/10.1007/s10898-020-00937-5},
volume = {79},
year = {2021}
}


@article{BOLTON2004549,
title = {The application of a unified Bayesian stopping criterion in competing parallel algorithms for global optimization},
journal = {Computers \& Mathematics with Applications},
volume = {48},
number = {3},
pages = {549-560},
year = {2004},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2003.09.030},
url = {https://www.sciencedirect.com/science/article/pii/S0898122104840768},
author = {H.P.J. Bolton and A.A. Groenwold and J.A. Snyman},
keywords = {Competing optimization algorithms, Global stopping criterion, Parallel optimization},
abstract = {The unconstrained global programming problem is addressed using a multistart, multialgorithminfrastructure, in which different algorithms compete in parallel for a contribution towards a single global stopping criterion, denoted the unified Bayesian global stopping criterion. The use of different algorithms is motivated by the observation that no single (global) optimization algorithm consistently outperforms all other algorithms when applied to large sets of problems from different classes. The Bayesian stopping criterion is based on the single assumption that the probability of each algorithm converging to the global optimum is at least as large as the probability of convergence to any other local minimum. This assumption is often valid in the case of practical problems of physical origin (e.g., determining physical configurations corresponding to minimum potential energy). Results for parallel clusters of up to 128 machines are presented.}
}


@article{Peri2012AMG,
  title={A multistart gradient-based algorithm with surrogate model for global optimization},
  author={Daniele Peri and Federica Tinti},
  journal={Communications in Applied and Industrial Mathematics},
  doi={10.1685/JOURNAL.CAIM.393},
  year={2012},
  volume={3}
}


@misc{multistartMATLAB,
  author = {},
  title = {MultiStart (Copyright 2009-2016 The MathWorks, Inc.)},
  year = {},
  url = {https://it.mathworks.com/help/gads/multistart.html},
  urldate = {2022-02-02}
}



@article{BetroSchoen1987,
title = {Sequential stopping rules for the multistart algorithm in global optimisation},
journal = {Mathematical Programming},
volume = {38},
number = {3},
pages = {271-286},
year = {1987},
issn = {1436-4646},
doi = {10.1007/BF02592015},
url = {https://doi.org/10.1007/BF02592015},
author = {Betrò, Bruno and Schoen, Fabio},
keywords = {},
abstract = {In this paper a sequential stopping rule is developed for the Multistart algorithm. A statistical model for the values of the observed local maxima of an objective function is introduced in the framework of Bayesian non-parametric statistics. A suitablea-priori distribution is proposed which is general enough and which leads to computationally manageable expressions for thea-posteriori distribution. Sequential stopping rules of thek-step look-ahead kind are then explicitly derived, and their numerical effectiveness compared.}
}




@article{BetroSchoen1992,
title = {Optimal and sub-optimal stopping rules for the Multistart algorithm in global optimization},
journal = {Mathematical Programming},
volume = {57},
number = {1},
pages = {445-458},
year = {1992},
issn = {1436-4646},
doi = {10.1007/BF01581094},
url = {https://doi.org/10.1007/BF01581094},
author = {Betrò, Bruno and Schoen, Fabio},
keywords = {},
abstract = {In this paper the problem of stopping the Multistart algorithm for global optimization is considered. The algorithm consists of repeatedly performing local searches from randomly generated starting points. The crucial point in this algorithmic scheme is the development of a stopping criterion; the approach analyzed in this paper consists in stopping the sequential sampling as soon as a measure of the trade-off between the cost of further local searches is greater than the expected benefit, i.e. the possibility of discovering a better optimum.}
}


@article{Piccioni1990,
author = {Piccioni, Mauro and Ramponi, Alessandro},
year = {1990},
month = {01},
pages = {697-707},
title = {Stopping rules for the multistart method when different local minima have different function values},
volume = {21},
journal = {Optimization},
doi = {10.1080/02331939008843596}
}


@article{Zielinski1981,
author = {Zieli\'{n}ski, Ryszard},
year = {1981},
month = {12},
pages = {348-356},
title = {A statistical estimate of the structure of multi-extremal problems},
volume = {21},
journal = {Mathematical Programming},
doi = {10.1007/BF01584254}
}









##### TEST FUNCTIONS:

#### HIMMELBLAU
@book{Himmelblau1972,
  title={Applied nonlinear programming},
  author={Himmelblau, D.M.},
  url={https://books.google.at/books?id=KMpEAAAAIAAJ},
  year={1972},
  publisher={McGraw-Hill}
}

#### ROSENBROCK 2D
@article{Rosenbrock1960,
    author = {Rosenbrock, H. H.},
    title = "{An Automatic Method for Finding the Greatest or Least Value of a Function}",
    journal = {The Computer Journal},
    volume = {3},
    number = {3},
    pages = {175-184},
    year = {1960},
    month = {01},
    abstract = "{The greatest or least value of a function of several variables is to be found when the variables are restricted to a given region. A method is developed for dealing with this problem and is compared with possible alternatives. The method can be used on a digital computer, and is incorporated in a program for Mercury.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/3.3.175},
    url = {https://doi.org/10.1093/comjnl/3.3.175},
    eprint = {https://academic.oup.com/comjnl/article-pdf/3/3/175/988633/030175.pdf},
}

#### EXTENDED (nD) ROSENBROCK
@article{ShangEtal20006_ROSENBROCKext,
    author = {Shang, Yun-Wei and Qiu, Yu-Huang},
    title = {{A Note on the Extended Rosenbrock Function}},
    journal = {Evolutionary Computation},
    volume = {14},
    number = {1},
    pages = {119-126},
    year = {2006},
    month = {03},
    abstract = {The Rosenbrock function is a well-known benchmark for numerical optimization problems, which is frequently used to assess the performance of Evolutionary Algorithms. The classical Rosenbrock function, which is a two-dimensional unimodal function, has been extended to higher dimensions in recent years. Many researchers take the high-dimensional Rosenbrock function as a unimodal function by instinct. In 2001 and 2002, Hansen and Deb found that the Rosenbrock function is not a unimodal function for higher dimensions although no theoretical analysis was provided. This paper shows that the n-dimensional (n = 4 ∼ 30) Rosenbrock function has 2 minima, and analysis is proposed to verify this. The local minima in some cases are presented. In addition, this paper demonstrates that one of the “local minima” for the 20-variable Rosenbrock function found by Deb might not in fact be a local minimum.},
    issn = {1063-6560},
    doi = {10.1162/evco.2006.14.1.119},
    url = {https://doi.org/10.1162/evco.2006.14.1.119},
    eprint = {https://direct.mit.edu/evco/article-pdf/14/1/119/1493646/evco.2006.14.1.119.pdf},
}


### NATURE-INSPIRED ALGORITHMS

@article{NatInspired_Opt_Yang2020,
archivePrefix = {arXiv},
arxivId = {2003.03776},
author = {Yang, Xin She},
doi = {10.1016/j.jocs.2020.101104},
eprint = {2003.03776},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Algorithm,Bat algorithm,Convergence,Cuckoo search,Differential evolution,Firefly algorithm,Flower pollination algorithm,Metaheuristic,Nature-inspired computation,Optimization,Particle swarm optimization,Stability,Swarm intelligence},
pages = {101104},
publisher = {Elsevier B.V.},
title = {{Nature-inspired optimization algorithms: Challenges and open problems}},
url = {https://doi.org/10.1016/j.jocs.2020.101104},
volume = {46},
year = {2020}
}



### GENETIC ALGORITHMS 

@book{GA_Mitchell1998,
author = {Mitchell, Melanie},
pages = {158},
title = {{Elements of Generic Algorithms - An Introduction to Generic Algorithms}},
url = {https://mitpress.mit.edu/books/introduction-genetic-algorithms},
year = {1998}
}

@article{GA_Yadav2012,
author = {Yadav, Pushpendra Kumar and Prajapati, N L},
number = {9},
pages = {1--4},
title = {{An Overview of Genetic Algorithm and Modeling}},
volume = {2},
year = {2012}
}


@incollection{GA_YANG202191,
title = {Chapter 6 - Genetic Algorithms},
editor = {Xin-She Yang},
booktitle = {Nature-Inspired Optimization Algorithms (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {91-100},
year = {2021},
isbn = {978-0-12-821986-7},
doi = {https://doi.org/10.1016/B978-0-12-821986-7.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219867000135},
author = {Xin-She Yang},
keywords = {Algorithm, crossover, elitism, genetic algorithm, mutation, optimization, selection},
abstract = {Genetic algorithm is one of the first evolutionary algorithms, which paces the way for contemporary evolutionary computation. This chapter introduces the basic principle of the genetic algorithm and its fundamental genetic operators: crossover, mutation, and selection.}
}



### PARTICLE SWARM OPTIMIZATION

@INPROCEEDINGS{PSO_1995,
author={Kennedy, J. and Eberhart, R.},
booktitle={Proceedings of ICNN'95 - International Conference on Neural Networks}, 
title={Particle swarm optimization}, 
year={1995},
volume={4},
number={},
pages={1942-1948 vol.4},
doi={10.1109/ICNN.1995.488968}
}


@incollection{PSO_YANG2021111,
title = {Chapter 8 - Particle Swarm Optimization},
editor = {Xin-She Yang},
booktitle = {Nature-Inspired Optimization Algorithms (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {111-121},
year = {2021},
isbn = {978-0-12-821986-7},
doi = {https://doi.org/10.1016/B978-0-12-821986-7.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219867000159},
author = {Xin-She Yang},
keywords = {Inertia weight, multi-agent system, optimization, particle swarm optimization, swarm intelligence},
}

@article{PSOstudy_socompsci2020,
title = {Sensitivity analysis of control parameters in particle swarm optimization},
journal = {Journal of Computational Science},
volume = {41},
pages = {101086},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101086},
url = {https://www.sciencedirect.com/science/article/pii/S1877750319300067},
author = {Mewael Isiet and Mohamed Gadala},
keywords = {Particle swarm optimization, Sensitivity analysis, Control parameters, Constrained optimization, Optimal parameter set, Metaheuristics},
}




### TEST FUNCTIONS

@MISC{AlRoomi2015,
author = {Ali R. Al-Roomi},
title = {{Unconstrained Single-Objective Benchmark Functions Repository}},
year = {2015},
address = {Halifax, Nova Scotia, Canada},
institution = {Dalhousie University, Electrical and Computer Engineering},
url = {https://www.al-roomi.org/benchmarks/unconstrained}
}


### MULTIPLE GRADIENT DESCENT

@article{Fliege2000,
author = {Fliege, J{\"{o}}rg and Svaiter, Benar Fux},
doi = {10.1007/s001860000043},
issn = {14322994},
journal = {Mathematical Methods of Operations Research},
keywords = {Multi-objective programming,Multicriteria optimization,Pareto points,Steepest descent,Vector optimization},
number = {3},
pages = {479--494},
title = {{Steepest descent methods for multicriteria optimization}},
volume = {51},
year = {2000}
}

@article{Schaffler2002,
author = {Sch{\"{a}}ffler, S. and Schultz, R. and Weinzierl, K.},
doi = {10.1023/A:1015472306888},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Brownian motion,curves of dominated points,stochastic differential equations,vector optimization problems},
number = {1},
pages = {209--222},
title = {{Stochastic method for the solution of unconstrained vector optimization problems}},
volume = {114},
year = {2002}
}


@techreport{Desideri2009,
author = {D{\'{e}}sid{\'{e}}ri, Jean Antoine},
institution = {HAL inria},
title = {{Multiple-Gradient Descent Algorithm (MGDA)}},
url = {https://inria.hal.science/inria-00389811},
year = {2009}
}


@techreport{Desideri2012_MGDA2,
author = {D{\'{e}}sid{\'{e}}ri, Jean Antoine},
number = {April},
title = {{MGDA II: A direct method for calculating a descent direction common to several criteria}},
url = {https://inria.hal.science/hal-00685762},
year = {2012}
}


@article{Desideri2012,
author = {D{\'{e}}sid{\'{e}}ri, Jean Antoine},
doi = {10.1016/j.crma.2012.03.014},
issn = {1631073X},
journal = {Comptes Rendus Mathematique},
number = {5-6},
pages = {313--318},
publisher = {Elsevier Masson SAS},
title = {{Multiple-gradient descent algorithm (MGDA) for multiobjective optimization}},
url = {http://dx.doi.org/10.1016/j.crma.2012.03.014},
volume = {350},
year = {2012}
}

@article{Peitz2018,
archivePrefix = {arXiv},
arxivId = {1612.03815},
author = {Peitz, Sebastian and Dellnitz, Michael},
doi = {10.1007/978-3-319-64063-1_7},
eprint = {1612.03815},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {159--182},
title = {{Gradient-based multiobjective optimization with uncertainties}},
volume = {731},
year = {2018}
}


@incollection{ROBBINS1971,
title = {{A Convergence Theorem for Non Negative Almost Supermartingales and some Applications}},
editor = {Jagdish S. Rustagi},
booktitle = {Optimizing Methods in Statistics},
publisher = {Academic Press},
pages = {233-257},
year = {1971},
isbn = {978-0-12-604550-5},
doi = {https://doi.org/10.1016/B978-0-12-604550-5.50015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780126045505500158},
author = {Robbins, H. and Siegmund, D.},
}



# AGGIUNTE DI STEFANIA PER ALTERNATE DESCENT IN MULTI-TASK


@article{mercierdesideri2018,
title = {A stochastic multiple gradient descent algorithm},
journal = {European Journal of Operational Research},
volume = {271},
number = {3},
pages = {808-817},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.05.064},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718304831},
author = {Mercier, Quentin and Poirion, Fabrice and Désidéri, {Jean-Antoine}},
}


@article{vicenteannor2021,
author = {Liu, Suyun and Vicente, {Luis Nunes}},
title = {{The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning}},
doi = {10.1007/s10479-021-04033-z},
journal = {Annals of Operations Research},
year = {2021},
url = {https://doi.org/10.1007/s10479-021-04033-z}
}


@article{vicentecms2022,
author = {Liu, Suyun and Vicente, {Luis Nunes}},
title = {{Accuracy and fairness trade-offs in machine learning: a stochastic multi-objective approach}},
doi = {10.1007/s10287-022-00425-z},
journal = {Computational Management Science},
year = {2022},
url = {https://doi.org/10.1007/s10287-022-00425-z},
volume = {19},
issue = {3},
pages = {513--537}
}


@article{vicentejota2023,
author = {Liu, Suyun and Vicente, {Luis Nunes}},
title = {{Convergence Rates of the Stochastic Alternating Algorithm for Bi-Objective Optimization}},
doi = {10.1007/s10957-023-02253-w},
volume = {198},
pages = {165--186},
journal = {Journal of Optimization Theory and Applications},
year = {2023},
url = {https://doi.org/10.1007/s10957-023-02253-w}
}



